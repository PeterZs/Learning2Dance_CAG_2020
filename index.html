
<!DOCTYPE html>
<html lang="en">
<head>
<br/>
<br/>
<title>Learning to Dance</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
<!-- jQuery -->
<script src="http://code.jquery.com/jquery.min.js"></script>
<!-- Bootstrap -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
<!-- Bootstrap -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<link rel="stylesheet" type="text/css" href="http://people.eecs.berkeley.edu/~shiry/styles/main.css"/>


<!-- ICONS -->
<link rel="icon" type="image/png" href="assets/icons/verlab_mini.png"/>
<link rel="icon" type="image/png" href="https://www.verlab.dcc.ufmg.br/wp-content/uploads/2013/09/SVG_Verlab_favicon900dpi-150x150.png/">
</head>
<body>

<div class="container">
<div class="row">
	<div class="col-md-12">
		<div class="page-header">
			<h1 class="text-center">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</h1>
		</div>
	</div>
</div>
<br>
<div class="row"> <!-- names row-->
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://jpeumesmo.github.io/">João Pedro Moreira Ferreira</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://thiagomcoutinho.github.io/"></a>Thiago Coutinho</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center">Thiago Luange</h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="http://netolcc06.github.io/">José Neto</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center">Rafael Vieira</h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center">Renato Martins</h5>
		</div>
		<div class="col-md-12">
			<h5 class="text-center"><a href="https://homepages.dcc.ufmg.br/~erickson/">Erickson Nascimento</a></h5>
		</div>
</div>
<br>
<div class="row">
	<div class="col-md-2">
	</div>
	<div class="col-md-5">
		<h5 class="text-right">Federal University of Minas Gerais</h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-center">INRIA</h5>
	</div>
	<div class="col-md-2">
	</div>
</div>

<br>
<div class="row">
	<div class="col-md-12">
		<h5 class="text-center">Computer & Graphics</h5>
	</div>
<div>
<br>
<div class="row">
	<div class="col-md-5">
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/verlab/2020-cag-ferreira-learningtodance-code">[Code]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/verlab/2020-cag-ferreira-learningtodance-code/blob/master/dataset.md">[Data]</a></h3>
	</div>
	<div class="col-md-5">
	</div>
<div>

<div class="row">
	<div class="col-md-1">
		<!--left margin column-->
	</div>

	<div class="col-md-10 text-center"> <!--main content column-->

	<p>
	    <figure class="figure">
	      <img src="assets/learning_to_dance.png" class="img-fluid mx-auto" alt="XXX">
	      <figcaption class="figure-caption">Learning to Dance, a framework to generate human motion from an audio input using Graph Convolutional Networks.</figcaption>
	    </figure>
	</p>

	<br>
    <h2>Abstract</h2>
     <br>
    <p class="text-justify">
		Synthesizing human motion through learning techniques is becoming an increasingly popular approach to alleviating the requirement of new data capture to produce animations. Learning to move naturally from audio, and in particular to dance, is one of the more complex motions humans often perform effortlessly. Each dance movement is unique, yet such movements maintain the core characteristics of the dance style. Most approaches addressing this problem with classical convolutional and recursive neural models undergo training and variability issues due to the non-Euclidean geometry of the motion manifold structure.In this paper, we design a novel method based on graph convolutional networks to tackle the problem of automatic dance generation from audio information. Our method is capable of generating natural motions, preserving the music style key moves across different generated motion samples, by taking advantage of an adversarial learning scheme conditioned on the input music audios. We evaluate our method using a user study and with several quantitative metrics of the generated motions' distributions. The results suggest the proposed GCN model outperforms the current state of the art dance generation method conditioned on music in different experiments. Moreover, our graph-convolutional approach is simpler, easier to be trained, and generates more realistic motion styles regarding qualitative and different quantitative metrics than state of the art, and with a visual movement perceptual quality even comparable to real motion data.
    </p>

	<br>
 	<hr>
 	<br>

 <h2>Paper</h2>
  <br>
    <ul class="media-list, citations">
    <li class="media" id="gestures">
        <a class="pull-left" href="https://arxiv.org/abs/1906.04160">
            <img class="media-object img-fluid img-thumbnail mr-4" src="assets/paper.png" alt="gestures paper" width="150">
        </a>
        <div class="media-body">
            <h5 class="media-heading text-left">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</h5>
            <p class="text-left">
			  João P. M. Ferreira, Thiago Coutinho, Thiago Luange, Jośe neto, Rafael Vieira, Renato Martins and Erickson Nascimento. 
              <span class="cite-title">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</span>,
              Elsevier Computer and Graphics, C&A, 2020.
              <br>
              <a href="https://arxiv.org/abs/1906.04160">PDF</a>, <a href="#gestures" role="button" data-toggle="collapse" data-target="#collapse-gestures" aria-expanded="false" aria-controls="collapse">BibTeX</a>
              <div class="collapse" id="collapse-gestures">
              <div class="card text-left bg-light mb-4">
                @InProceedings{ginosar2019gestures,<br> 
        &nbsp;&nbsp;author={S. Ginosar and A. Bar and G. Kohavi and C. Chan and A. Owens and J. Malik},<br> 
        &nbsp;&nbsp;title = {Learning Individual Styles of Conversational Gesture},<br>
        &nbsp;&nbsp;booktitle = {Computer Vision and Pattern Recognition (CVPR)}<br> 
        &nbsp;&nbsp;publisher = {IEEE},<br>
        &nbsp;&nbsp;year={2019},<br> 
        &nbsp;&nbsp;month=jun<br>
        }
              </div>
              </div>
            </p>
        </div>
    </li>
  </ul>

    <br>
 	<hr>
 	<br>

 	<h2>Demo</h2>
 	<br>
 	<iframe width="560" height="315" src="assets/learning_to_dance.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

  	<br>
 	<hr>
 	<br>

	<h2>Code</h2>
	<p>
	<h3 class="text-center"><a href="https://github.com/verlab/2020-cag-ferreira-learningtodance-code">[PyTorch]</a></h3>
	</p>

 	<br>
 	<hr>
 	<br>

   	 <h2>Data</h2>
   	 <br>
   	 <div>
   	    <img src="assets/dataset.png" alt="Dataset" width=auto>
 	</div>
	<br>
   	 <p class="text-justify">
   	 	We present a large, 144-hour person-specific video dataset of 10 speakers, with frame-by-frame automatically-detected pose annotations. We deliberately pick a set of speakers for which we can find hours of clean single-speaker footage. Our speakers come from a diverse set of backgrounds: television show hosts, university lecturers and televangelists. They span at least three religions and discuss a large range of topics from commentary on current affairs through the philosophy of death, chemistry and the history of rock music, to readings in the Bible and the Qur'an.
   	 </p>
   	 <p class="text-justify">
   	 	Note: the data for Conan was updated recently to remove duplicate videos. The numerical results pertaining to Conan will be updated soon. 
   	 </p>
   	<div>
   		<a href="https://github.com/verlab/2020-cag-ferreira-learningtodance-code/blob/master/dataset.md" class="btn btn-outline-secondary">Download</a>
   	</div>

   	<br>
 	<hr>
 	<br>

   	 <!-- <h2>Press Coverage</h2>
   	 <br>
   	 <div class="row">
    	<div class="col-md-12">
      		<a href="https://www.sciencemag.org/news/2019/06/watch-artificial-intelligence-predict-conan-o-brien-s-gestures-just-sound-his-voice">
        	<img class="img-responsive" src="images/press/science.jpg" alt="sciencemag logo" width="100">
      		</a>
    	</div>
    </div> -->
   	 

    <!-- <br>
 	<hr>
 	<br> -->

 	 <h2>Acknowledgements</h2>
 	  <br>
 	 <p class="text-justify">
		  This work was supported, in part, by Nvidia, Petrobras, Cnpq, Capes and Fapemig. 
 	 </p>

	</div> <!-- close middle column -->
	<div class="col-md-1">
	<!-- empty room saver on right -->
	</div>
</div> <!-- close main row -->
</div> <!-- close body container --> 
<footer>
  <br/>
  <br/>
</footer>
</div>
</body>
</html>

